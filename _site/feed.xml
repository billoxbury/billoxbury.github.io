<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-25T09:37:53+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Bills.Data</title><subtitle>Exploring ideas in data science</subtitle><author><name>Bill Oxbury</name></author><entry><title type="html">Language barriers in global conservation</title><link href="http://localhost:4000/environment/language_barriers/" rel="alternate" type="text/html" title="Language barriers in global conservation" /><published>2023-02-22T00:00:00+00:00</published><updated>2023-02-22T00:00:00+00:00</updated><id>http://localhost:4000/environment/language_barriers</id><content type="html" xml:base="http://localhost:4000/environment/language_barriers/">&lt;p&gt;&lt;img src=&quot;&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2023-02-22/birdclouds.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Wildlife tends not to respect national boundaries. Birds, in particular, as they migrate across and between continents, ignore not only borders but even the cultures and languages of the scientists who may be trying to study and protect them. And, surprise surprise, not all of the world’s science is written in English.&lt;/p&gt;

&lt;p&gt;According to a recent study [1], more than 30% of scientific articles on biodiversity conservation are written in non-English languages. Moreover, the authors claim that non-English language scientific outputs – at least within biodiversity conservation – are increasing both in volume and in quality. In geographic regions where English is not widely used – as in some of the world’s biodiversity hotspots – key data and evidence are generated by local scientists and even by citizen science projects in local languages. So when it comes to tracking species with global geographic ranges, restricting only to English-language scientific outputs can lead to key gaps in knowledge.&lt;/p&gt;

&lt;p&gt;The study [2] takes a closer look at this issue for bird species. They compare the known geographic ranges of more than 10,000 species with the official languages listed for the countries covered by those ranges. They show that more than 1,500 species have coverage of at least 10 languages. High numbers of ‘multi-lingual’ species have ranges spanning Eastern Europe, Russia and central Asia. Nevertheless, they also observe that four European languages – English, Spanish, Portuguese and French – dominate species coverage globally, each reaching between 3,000 and 6,000 bird species.&lt;/p&gt;

&lt;p&gt;In a project for &lt;a href=&quot;https://www.birdlife.org/&quot; target=&quot;_blank&quot;&gt;BirdLife International&lt;/a&gt;, I have been able to put some of these observations to the test, from the complementary perspective of what can be seen directly in the scientific literature. &lt;i&gt;LitScan&lt;/i&gt; is a system to crawl and identify scientific articles of relevance to Red List assessments. It scans various sources across multiple languages; makes use of &lt;a href=&quot;https://spacy.io/&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; for text-processing (including language id, discovery of species mentions and conservation relevance); and uses Cloud cognitive services for translation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2023-02-22/Picus_viridis.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the purposes of this blog post – and for comparison with the results of [2] – I want to focus on just one of the &lt;i&gt;LitScan&lt;/i&gt; sources &lt;a href=&quot;https://openalex.org/&quot; target=&quot;_blank&quot;&gt;OpenAlex&lt;/a&gt; [3]. This is an extremely useful open-source repository of metadata for scientific documnents drawing on an impressive &lt;a href=&quot;https://openalex.org/about&quot; target=&quot;_blank&quot;&gt;range of sources&lt;/a&gt;. I don’t know what the exact language coverage of OpenAlex is (and I’m sure it could be improved), but I can make some observations based on &lt;i&gt;LitScan&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;(Incidentally, besides OpenAlex, &lt;i&gt;LitScan&lt;/i&gt; taps directly into various non-English sources. These are quite specific and would bias any comparison with [2], so in this post I’ll just restrict to &lt;i&gt;LitScan&lt;/i&gt; data that comes from OpenAlex.)&lt;/p&gt;

&lt;p&gt;So here’s a data set – used by &lt;i&gt;LitScan&lt;/i&gt; but constructed as follows. Over a 3-month period a daily request was made to OpenAlex. The request consisted of 500 searches, each on the scientific name of a bird species drawn at random from a list of 11,188. The searches are not all successful, and over the collection period the number of documents returned – after some additional filtering for conservation relevance and publication since the year 2000 – was 35,303 (so averaging about 400 per day).&lt;/p&gt;

&lt;p&gt;The total number of species covered by these documents was 3,517, in a total of 32 languages – by far dominated by English (32,239 documents), with the next most numerous language being Spanish (824 documents).&lt;/p&gt;

&lt;p&gt;We can now ask, in the spirit of [2]: &lt;b&gt;how many species&lt;/b&gt; are found in &lt;b&gt;non-English documents only&lt;/b&gt;? Moreover, since we are interested in conservation relevance, we can ask for this number broken down by red-list status – as defined by the &lt;a href=&quot;https://www.iucnredlist.org/&quot; target=&quot;_blank&quot;&gt;IUCN Red List of Threatened Species&lt;/a&gt; – as well as language:&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&quot;text-align:left;&quot;&gt;   &lt;/th&gt;
   &lt;th style=&quot;text-align:right;&quot;&gt; LC &lt;/th&gt;
   &lt;th style=&quot;text-align:right;&quot;&gt; NT &lt;/th&gt;
   &lt;th style=&quot;text-align:right;&quot;&gt; VU &lt;/th&gt;
   &lt;th style=&quot;text-align:right;&quot;&gt; EN &lt;/th&gt;
   &lt;th style=&quot;text-align:right;&quot;&gt; CR &lt;/th&gt;
   &lt;th style=&quot;text-align:right;&quot;&gt; EX &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Spanish &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 211 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 13 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 14 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 2 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Portuguese &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 52 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 11 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 7 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 5 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Indonesian &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 29 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 11 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 2 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; French &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 25 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 8 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 3 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; German &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 5 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Korean &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 4 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Mandarin &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 2 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Czech &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Catalan &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 2 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Norwegian &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;text-align:left;&quot;&gt; Croatian &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
   &lt;td style=&quot;text-align:right;&quot;&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(The columns are the red-list categories &lt;i&gt;Least Concern&lt;/i&gt;, &lt;i&gt;Near Threatened&lt;/i&gt;, &lt;i&gt;VUlnerable&lt;/i&gt;, &lt;i&gt;ENdangered&lt;/i&gt;, &lt;i&gt;CRitically endangered&lt;/i&gt; and &lt;i&gt;EXtinct&lt;/i&gt;.)&lt;/p&gt;

&lt;p&gt;These are small numbers, but every species counted in this table represents information that may be lost to red-list assessors who have access only to English-language science. Moreover, many more species are represented in both English and non-English documents (and so are not counted here).&lt;/p&gt;

&lt;p&gt;The document sampling using OpenAlex is far from unbiased – clearly we have not tapped into a much wider literature in, say, Mandarin or Korean. The &lt;i&gt;LitScan&lt;/i&gt; ambition is to maximise the use of sources in those languages directly in the future. Nevertheless, the analysis offers an interesting corroboration of the observations in [1,2].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;T. Amano et al: &lt;a href=&quot;https://doi.org/10.1371/journal.pbio.3001296&quot; target=&quot;_blank&quot;&gt;Tapping into non-English language science for conservation of global biodiversity&lt;/a&gt;, PLOS Biology (2021) &lt;i&gt;doi: 10.1371/journal.pbio.3001296&lt;/i&gt;&lt;/li&gt;
  &lt;li&gt;Pablo Jose Negret, Scott C. Atkinson, Bradley K. Woodworth, Marina Corella Tor, James R. Allan, Richard A. Fuller, Tatsuya Amano: &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0267151&quot; target=&quot;_blank&quot;&gt;Language barriers in global bird conservation&lt;/a&gt; PLOS One (2022) &lt;i&gt;doi: 10.1371/journal.pone.0267151&lt;/i&gt;&lt;/li&gt;
  &lt;li&gt;Priem, J., Piwowar, H., &amp;amp; Orr, R. (2022). &lt;a href=&quot;https://openalex.org/&quot; target=&quot;_blank&quot;&gt;OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts&lt;/a&gt; (2022) &lt;i&gt;arXiv: &lt;a href=&quot;https://arxiv.org/abs/2205.01833&quot; target=&quot;_blank&quot;&gt;arxiv.org/abs/2205.01833&lt;/a&gt;&lt;/i&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Bill Oxbury</name></author><category term="environment" /><summary type="html"></summary></entry><entry><title type="html">Navigating the PLOS ONE topic tree</title><link href="http://localhost:4000/data_science/plosone-topic-tree/" rel="alternate" type="text/html" title="Navigating the PLOS ONE topic tree" /><published>2022-02-10T00:00:00+00:00</published><updated>2022-02-10T00:00:00+00:00</updated><id>http://localhost:4000/data_science/plosone-topic-tree</id><content type="html" xml:base="http://localhost:4000/data_science/plosone-topic-tree/">&lt;p&gt;&lt;img src=&quot;&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-10/browse_topics.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PLOS ONE is a respected multidisciplinary journal publishing research from&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;over two hundred subject areas across science, engineering, medicine, and the related social sciences and humanities.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But exactly &lt;strong&gt;how many&lt;/strong&gt; subject areas?&lt;/p&gt;

&lt;p&gt;At the top level (as shown in the screenshot above), these subject areas fall under eleven headings from &lt;em&gt;Biology and life sciences&lt;/em&gt; to &lt;em&gt;Social sciences&lt;/em&gt;. For each of these headings – such as &lt;em&gt;Computer and information sciences&lt;/em&gt; below - we are told the number of articles (in this case 32,397) and can browse further subheadings:&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;/assets/img/2022-02-10/compsci_count.png&quot; alt=&quot;&quot; /&gt;
    
  
    
      &lt;img src=&quot;/assets/img/2022-02-10/compsci_topics.png&quot; alt=&quot;&quot; /&gt;
    
  
  
&lt;/figure&gt;

&lt;p&gt;This post is about a short exercise to scan the entire tree of PLOS ONE topics, asking:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;how does one extract and represent this tree?&lt;/li&gt;
  &lt;li&gt;are the article counts per topic &lt;em&gt;consistent&lt;/em&gt; e.g. in the sense that each count is the sum of the counts at the leaves of the corresponding subtree?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Visually, the result of the analysis is plots such as the following (the topic trees of some of the top-level headings):&lt;/p&gt;

&lt;embed src=&quot;/assets/img/2022-02-10/seven_trees.pdf&quot; type=&quot;application/pdf&quot; frameborder=&quot;0&quot; scrolling=&quot;auto&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;

&lt;h2&gt;1. Crawling&lt;/h2&gt;

&lt;p&gt;The starting task was to crawl the PLOS ONE pages. To do this, we initialise a data frame with a single row (I’ll use Python-like pseudocode throughout - most of this exercise was actually done in R):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;topic_tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;node&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;s&quot;&gt;&apos;parent&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;s&quot;&gt;&apos;topic&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;s&quot;&gt;&apos;parent_topic&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;s&quot;&gt;&apos;count&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;… and then add tree nodes to the data frame via the following breadth-first search:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;browse_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://journals.plos.org/plosone/browse/&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;this_topic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic_tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;topic&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;browse_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_topic&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;topic_tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;count&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;find_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# read HTML
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;next_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;find_children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                    &lt;span class=&quot;c1&quot;&gt;# read HTML    
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;child&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;topic_tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;node&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;
                          &lt;span class=&quot;s&quot;&gt;&apos;parent&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                          &lt;span class=&quot;s&quot;&gt;&apos;topic&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;child&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;s&quot;&gt;&apos;parent_topic&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_topic&lt;/span&gt;
                          &lt;span class=&quot;s&quot;&gt;&apos;count&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
                          &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this code, the functions &lt;tt&gt;find_count()&lt;/tt&gt; and &lt;tt&gt;find_children()&lt;/tt&gt; parse the HTML of the currently visited topic page and extract the count and subtopics respectively. (For this I use the package &lt;em&gt;rvest&lt;/em&gt; in R.)&lt;/p&gt;

&lt;h2&gt;2. Counting&lt;/h2&gt;

&lt;p&gt;The code generates a data frame with &lt;i&gt;(as of 9 Feb 2022)&lt;/i&gt; a total of 16,721 rows (topics). From this data frame we can read off any tree statistics - for example the node count by depth in the tree:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-10/count_by_depth.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To illustrate the data frame, the following slice is the part of the tree below node 49, &lt;i&gt;sports_science&lt;/i&gt;. (This is an example of a &lt;em&gt;clade&lt;/em&gt;: a subtree that exactly consists of one node and all its descendants:)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-10/sports_tree0.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-02-10/sports_tree1.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first thing to note is that the article counts (as read off from the PLOS ONE topic pages) are not in any sense consistent! (That was question 2 at the top of this post.)&lt;/p&gt;

&lt;p&gt;At each node, we can read off an &lt;strong&gt;excess count&lt;/strong&gt;, which is the difference between the advertised article count and the sum of the counts at child nodes. This excess is usually positive: for example, at the topic &lt;em&gt;exercise&lt;/em&gt; the excess is 1,515 - the number of articles that presumably do not fall under the subtopics of &lt;i&gt;aerobic_exercise&lt;/i&gt; or &lt;i&gt;strength_training&lt;/i&gt;. This is to be expected if the topic tree grows over time with new subtopics being added to the &lt;a href=&quot;https://github.com/PLOS/plos-thesaurus&quot; target=&quot;_blank&quot;&gt;PLOS thesaurus&lt;/a&gt;. On the other hand, the excess count is often negative. For example, the count at &lt;i&gt;sports_science&lt;/i&gt; is actually &lt;em&gt;less than&lt;/em&gt; the counts at its two subtopics &lt;i&gt;sports&lt;/i&gt; and &lt;i&gt;sports_and_exercise_medicine&lt;/i&gt;. At some point, the parent node has stopped counting!&lt;/p&gt;

&lt;p&gt;(It turns out that the excess count has a large negative value at all of the 11 top-level topics – which are therefore underestimating the number of articles they cover.)&lt;/p&gt;

&lt;h2&gt;3. Drawing&lt;/h2&gt;

&lt;p&gt;Finally, a word about tree formats and visualisation. The circular plots shown above could have been made using a package like R &lt;em&gt;phylotools&lt;/em&gt;. In fact, I took a shortcut and used the very convenient &lt;a href=&quot;https://itol.embl.de/&quot; target=&quot;_blank&quot;&gt;Interactive Tree Of Life (iTOL)&lt;/a&gt; site.&lt;/p&gt;

&lt;p&gt;In either case, a more compact data format is needed than the data frame shown above. A popular format that I used is the &lt;strong&gt;Newick format&lt;/strong&gt;. The idea of Newick format is that the tree is represented by a string with the recursive form&lt;/p&gt;

&lt;p&gt;\[
\nu({\rm tree}) = (\nu({\rm child}_1),\ldots,\nu({\rm child}_k))\nu({\rm root})
\]&lt;/p&gt;

&lt;p&gt;and where $\nu({\rm single\ node})$ is any convenient string representing that node, e.g. the topic name. Converting from the data frame shown above to Newick format is then achieved with a simple recursive function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;newickR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exist&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;(&quot;&lt;/span&gt;                           &lt;span class=&quot;c1&quot;&gt;# open bracket
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# insert commas-separated child strings
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newickR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;,&apos;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;,&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                  &lt;span class=&quot;c1&quot;&gt;# remove final comma
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;)&apos;&lt;/span&gt;                          &lt;span class=&quot;c1&quot;&gt;# close bracket
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;topic&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;# append parent string
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Applied to the little &lt;i&gt;sports_science&lt;/i&gt; data frame this outputs:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newickR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&apos;(sports,((aerobic_exercise,strength_training)exercise)sports_and_exercise_medicine)sports_science&apos;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name>Bill Oxbury</name></author><category term="data_science" /><summary type="html"></summary></entry><entry><title type="html">COP26: seeing the wood for the trees</title><link href="http://localhost:4000/environment/cop26-nzs/" rel="alternate" type="text/html" title="COP26: seeing the wood for the trees" /><published>2021-11-03T00:00:00+00:00</published><updated>2021-11-03T00:00:00+00:00</updated><id>http://localhost:4000/environment/cop26-nzs</id><content type="html" xml:base="http://localhost:4000/environment/cop26-nzs/">&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-03/woodfortrees1.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The climate crisis poses an almost overwhelming challenge to humanity, and it is very easy to be pessimistic. But there are also some reasons to be hopeful. Many will have been inspired by Prince William’s Earthshot initiative, by the vision behind it and the passion and creativity of the finalists and other innovators. It gives me genuine hope that we have the capacity to correct our course for a safe future.&lt;/p&gt;

&lt;p&gt;You will hear about many innovations and tech solutions for sustainability in the margins of COP26. However, the success of the conference will stand or fall on one thing only: the ability of the international community to agree a practical pathway to achieve the 1.5°C goal of the Paris agreement.&lt;/p&gt;

&lt;p&gt;This is ultimately a numbers game. One of the most hopeful conclusions from the 
&lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/#FullReport&quot;&gt;2021 IPCC 6th Assessment Report&lt;/a&gt;
 is that there is a strong linear relationship between the average global temperature we arrive at this century and the total volume of carbon that we emit into the atmosphere from today. In other words, the world has an emissions ‘budget’: the lower the budget, the lower the final temperature rise.&lt;/p&gt;

&lt;p&gt;To understand the budget choices, the following table is from the IPCC report:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-03/image1.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The table expresses uncertainties and margins of error. Sorry. But it’s the best science we have to go on, so let’s see what it says.&lt;/p&gt;

&lt;p&gt;The rows of the table express budgets (in GtCO2, or giga-tonnes of CO2 emission) for limiting the global temperature rise to values on the left-hand margin, with likelihood expressed in the top margin. So, for example, 500 GtCO2 buys a 50% chance of staying within 1.5°C in this century. Let’s go with that for the moment – assuming a 50% chance feels a safe enough bet for you.&lt;/p&gt;

&lt;p&gt;So how much is 500 GtCO2? For comparison, the world in 2020 released approximately 40 GtCO2 (with the UK responsible for about 0.3 GtCO2 of that). So at current rates, 500 equates to about 12 years. Or if we could assume a 7% worldwide reduction in emissions every year, then it buys us 30 years.&lt;/p&gt;

&lt;p&gt;(Two more comparisons: the proposed coking coal mine at Woodhouse Colliery would commit the UK to about 0.16 GtCO2 from the coal extracted over the lifetime of the mine; the Cambo oil field in the North Sea commits us to about 0.3 GtCO2. Both would eat significantly into any reasonable budget for the UK.)&lt;/p&gt;

&lt;p&gt;This is the numbers game that COP26 has to solve in order to ensure a safe future. What emissions budget can the international community agree that will set an acceptable level of temperature risk? Given that budget, how will it be divided equitably among nations? And how do we support poorer nations to live within their budget as they transition to a zero-carbon future?&lt;/p&gt;

&lt;p&gt;All too often, responding to the climate crisis is framed as being about personal choices (you should fly less, you should eat less meat). It isn’t. Ultimately, it depends on science-driven policies, tireless diplomacy and international cooperation to shift those big carbon numbers. COP26 is a critical part of that process.&lt;/p&gt;

&lt;p&gt;Yes, our lifestyles will change, and individual choices are important – but for the majority they’ll change the way they always have done: in response to better choices coming along through vision, investment and legislation.&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="environment" /><summary type="html"></summary></entry><entry><title type="html">COP26: why 1.5 degrees?</title><link href="http://localhost:4000/environment/cop26-why1pt5/" rel="alternate" type="text/html" title="COP26: why 1.5 degrees?" /><published>2021-11-02T00:00:00+00:00</published><updated>2021-11-02T00:00:00+00:00</updated><id>http://localhost:4000/environment/cop26-why1pt5</id><content type="html" xml:base="http://localhost:4000/environment/cop26-why1pt5/">&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-02/image1.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the 2015 Paris Agreement, countries of the world signed up to &lt;em&gt;“keep the rise in average global temperature to well below 2°C above pre-industrial levels, and preferably limit the increase to 1.5°C”&lt;/em&gt;.  This month’s COP26 summit is the last best chance for nations to agree plans of action to achieve that goal. For the reasons I discussed in the previous blog, it’s vital for all our futures that we succeed.&lt;/p&gt;

&lt;p&gt;So why 1.5°C? The figure sounds insignificant – what does it mean?&lt;/p&gt;

&lt;p&gt;Global average temperature is not a flat, uniform thing. It’s more like the surface of the sea – it has peaks and troughs. And actually, as the average rises, so does the variation between those peaks and troughs. The graphic above, taken from the &lt;a href=&quot;https://www.atlasoftheinvisible.com/&quot;&gt;Atlas of the Invisible&lt;/a&gt;, illustrates this. Each square is a little map of the Earth, showing average temperature from 1890 to 2019. The colour, from blue to red, shows how far the temperature is below or above the global average for the ‘baseline period’ (1960s to 1980s). What you see is not only the trend but the variation across the planet.&lt;/p&gt;

&lt;p&gt;So a total average temperature rise of 1.1°C (above the pre-industrial average, which is roughly where we are today), affects different parts of the planet in different ways, and has led to the big effects in terms of extreme weather, melting ice and changing rainfall patterns that we see today.&lt;/p&gt;

&lt;p&gt;Let’s put that figure of 1.1°C in historical context. The following graphic is taken from the &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/#FullReport&quot;&gt;IPCC’s 6th Assessment Report&lt;/a&gt; this summer. It shows that insignificant-looking temperature difference in the context of the last two thousand years. In this context, we see that 1.1°C is a very big deal and that the rise is not gradual but is better thought of as a shock to the system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-02/image2.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graphic also shows that today’s temperature is outside anything the planet has experienced in the past 100,000 years. At 1.5°C we will already head beyond any conditions that have existed during the lifetime of our species. Yet the IPCC report makes clear that we are very likely to exceed 1.5°C by 2040 and possibly by 2030. Without radical actions agreed at COP26, according to the IPCC, we are heading past 2.4°C by 2040 and to around 4.0°C by the end of the century.&lt;/p&gt;

&lt;p&gt;A temperature rise of 4.0°C is pretty unthinkable. In geological time it takes us back nearly 50 million years to an age before primates had evolved and when the poles were temperate. The sea level rise this would induce would drown all the major coastal cities in the world.&lt;/p&gt;

&lt;p&gt;A major risk of permitting uncontrolled temperature rise is the existence of tipping points in the Earth system – temperature levels at which global processes will kick in that would send the temperature rise higher still. The graphic below, by Tim Lenton, illustrates these tipping points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-02/image3.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So the threats are real and current climate change is very much a crisis. But it is still not too late in the day, and there is no doubt that humanity has the ingenuity and resourcefulness to rise to the challenge. Rising to the challenge is just what we have to look to COP26 to do.&lt;/p&gt;

&lt;p&gt;The problem to be solved is stated as a number, 1.5 degrees – so any solutions have to play a numbers game. I’ll talk about that in the last blog.&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="environment" /><summary type="html"></summary></entry><entry><title type="html">COP26: the climate crisis today</title><link href="http://localhost:4000/environment/cop26-today/" rel="alternate" type="text/html" title="COP26: the climate crisis today" /><published>2021-11-01T00:00:00+00:00</published><updated>2021-11-01T00:00:00+00:00</updated><id>http://localhost:4000/environment/cop26-today</id><content type="html" xml:base="http://localhost:4000/environment/cop26-today/">&lt;p&gt;&lt;img src=&quot;/assets/img/2021-11-01/image1.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the first of three blogs in which I’d like to share a few thoughts on the climate crisis and on the COP26 conference which launches in Glasgow this week. In the &lt;a href=&quot;/environment/cop26-why1pt5/&quot;&gt;second blog&lt;/a&gt; I’ll say a bit about the figure of 1.5 degrees warming and what it means; in the &lt;a href=&quot;/environment/cop26-nzs/&quot;&gt;third&lt;/a&gt; I’ll focus on COP itself.&lt;/p&gt;

&lt;p&gt;But let’s start with today. As we look back over the summer of 2021, we’ve seen a number of tragic events in the news: the Pacific North-West of the US and Canada was hit by a ‘heat dome’ killing more than 500 people – as well as up to a billion marine animals. Southern Europe was hit by record wildfires killing many people in Greece, Italy and Portugal. Wildfires in Siberia were reported to be larger than the rest of the world’s combined – as rain was observed on the summit of the Greenland icecap for the first time. During the same few weeks major floods hit China (302 deaths), Germany (196 deaths), Belgium (42 deaths), Mexico (17 deaths) and Pakistan (160 deaths). Back in the US, hurricane Ida travelled up the east coast from Louisiana to New York with a death toll of 82.&lt;/p&gt;

&lt;p&gt;This is just a snapshot of recent times. The climate crisis, and our response to it, is not just about ‘saving the planet’, it’s about saving lives. And extreme weather is not a ‘new normal’ – because the Earth system is in a state of transition and will take many decades, or even centuries, to settle into a new stable state.&lt;/p&gt;

&lt;p&gt;The climate crisis is also about national security. Our security depends not only on safety from wildfires and floods, but also on resilient infrastructure, a stable economy and secure food and water supplies. It depends on stable international relationships and supply chains and the ability of our people, from tourists and business people to diplomats and military personnel, to travel and operate safely overseas.&lt;/p&gt;

&lt;p&gt;The evolving threat around the world is not only from extreme weather events. It’s also from changing rainfall patterns and failing agriculture, from depleted water sources and from sea level rise and storm surges. It will drive tensions over these resources and over displaced populations.
The graphic at the top of this post, illustrating some of these cascading risks to security, is taken from a &lt;a href=&quot;https://www.chathamhouse.org/2021/09/climate-change-risk-assessment-2021&quot;&gt;recent Chatham House report&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&quot;https://imccs.org/the-world-climate-and-security-report-2021/&quot;&gt;World Climate and Security Report (WCSR) 2021&lt;/a&gt;, “within the next twenty years, security risks stemming from climate phenomena will present severe and catastrophic levels of risk”. That’s a stark message, and I shall offer a bit more optimism in the third of these blogs. But there is no doubt that as the crisis unfolds it will affect the politics, rivalries and stability of all nations.&lt;/p&gt;

&lt;p&gt;Examples cited in the WCSR report include: floods in Sudan in 2020 exacerbating the country’s already fragile security infrastructure, as well as requiring assistance from the Egyptian military for humanitarian aid; the impact of recent floods and wildfires on US military bases and equipment; and increased tensions in the Arctic. China’s economic dominance in the 21st century is argued to be “not guaranteed” because of the dense concentration of population and economic hubs in cities most at risk from sea level rise and from water shortages. Russia may perceive that it is less at risk from climate change than other countries, yet it is vulnerable both from the threat to infrastructure of melting permafrost, and from exposed fossil fuel assets if the world succeeds in shifting rapidly to renewable energy sources.&lt;/p&gt;

&lt;p&gt;These are just examples of the ways in which the climate crisis will have deep implications for behaviours and intents of nation states. So the COP26 conference matters to us. It is a milestone event which will have huge implications for the direction that humanity takes. I’ll say a bit about the problem it has to solve in the next two blogs.&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="environment" /><summary type="html"></summary></entry><entry><title type="html">Watching a neural network learn a Markov chain</title><link href="http://localhost:4000/data_science/rnn-learns-markov/" rel="alternate" type="text/html" title="Watching a neural network learn a Markov chain" /><published>2019-11-01T00:00:00+00:00</published><updated>2019-11-01T00:00:00+00:00</updated><id>http://localhost:4000/data_science/rnn-learns-markov</id><content type="html" xml:base="http://localhost:4000/data_science/rnn-learns-markov/">&lt;p&gt;Modern language models derive their power from big data and big compute – but also, ultimately, from the &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot; target=&quot;_blank&quot;&gt;Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt; described by Andrej Karpathy (and many others) a decade or so ago. This post is more low-brow than Karpathy’s — I wanted to explore a little bit how RNNs perform on some carefully controlled toy data: specifically on sequences generated from Markov chains.&lt;/p&gt;

&lt;p&gt;What does it mean to model a sequence? It means two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Given a new sequence generated by the same process, can we predict at each time step the next sequence item?&lt;/li&gt;
  &lt;li&gt;Does the ‘model’ give simplying insights into this underlying process?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Suppose, for example, that the sequence is drawn from an ‘alphabet’ of $N$ distinct symbols, and that the process generating the sequence is Markov — in other words, each item depends only on its predecessor. In that case, problem 1 is completely solved in $O(N^2)$ storage by just collecting enough data and observing conditional frequencies. This does nothing to address problem 2, however. On the other hand, fitting an $n$-state hidden Markov model (HMM) reduces the storage to $O(nN)$. So it improves the solution to problem 1 — but it also answers problem 2 if we can interpret what the ‘hidden states’ mean.&lt;/p&gt;

&lt;p&gt;However, most sequences generated by processes of interest are not 1-st order Markov. If the process is Markov of order $k$, then the storage requirement for the naive solution is $O(Nk)$, which rapidly becomes intractable. This is where RNNs come in. At least, they appear to perform well on problem 1. What about problem 2? Can we extract any understanding from them?&lt;/p&gt;

&lt;p&gt;Here’s an example. It’s a stretch of ASCII sequence (from a source which I’ll reveal in a moment):&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;
ikkviiviiekotkiwiwiieeiiioikttooiotkkeiiokkttkkiwvvwtkikweiwvwikkkiokkwookkkkoiikiieiiiwoivveiioikokiivikoiooookkikkvwveikookkutktktvvwktkkwwkiwiwikkuktkkoiwkkkkotewiikkiukkvwwktkkokkieeiiwiookkiiiiot
&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Let’s see what we can learn by fitting a recurrent network (of Long Short-Term Memory (LSTM) units).
Since the sequence only exhibits a small alphabet consisting of &lt;tt&gt;{e,i,k,o,t,u,v,w}&lt;/tt&gt;, I’m not going to expend a huge model on it, so I choose an RNN architecture consisting of just one hidden layer with &lt;strong&gt;2 units&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Having trained this small network, let’s observe the output of those two inner units — as points in the plane ${\mathbb R}^2$ — as we pass the sequence through it:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-11-01/ex1_2_2_1_bystate.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;What are the colours? Well, I now need to reveal that the sequence was generated from a 2-state hidden Markov model with states (where the coefficients are shorthand for probabilities of the state outputting each character):&lt;/p&gt;

&lt;p&gt;\[
0.496 {\tt k} + 0.302 {\tt o} + 0.157 {\tt t} + 0.045 {\tt u}
\]&lt;/p&gt;

&lt;p&gt;\[
0.106 {\tt e} + 0.562 {\tt i} + 0.117 {\tt v} + 0.215 {\tt w}
\]&lt;/p&gt;

&lt;p&gt;together with some randomly chosen $2\times 2$ transition matrix for moving between the states. I’ve coloured the plot above by the HMM state. What we see is that this internal structure is effectively discovered by the RNN. (So it gives us some help with question 2 above. I’ll come to question 1 in a moment.)&lt;/p&gt;

&lt;p&gt;This example was very easy because the HMM states are ‘far apart’: they output completely disjoint sets of characters. Let’s look at some other examples. The first does the same as above, but for data generated from a 3-state HMM with closer (overlapping) output distributions. The graph on the right represents the HMM states, with edges representing overlap (actually, closeness of the output distributions using their total variation):&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;
xrrxzzkkzzrxkrzzkkkkzrzoxzkoxrzzzkkzkoznzznznrxxrkkxxrxrrrkrznzxzznzrnnxxxxrrzkkznkkzknznnkzkkokkzzkrrkxknzzoozzzzzzzzzzzoknzzdrroznzzzzznznzznnrxokkxrrozkxrzzkkkrrrxxokkoozkokzzxookozzndzzokzkkxkzxrz
&lt;/tt&gt;&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex2_2_3_1_bystate.png&quot; alt=&quot;&quot; /&gt;
    
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex2_2_3_1_families.png&quot; alt=&quot;&quot; /&gt;
    
  
  
&lt;/figure&gt;

&lt;p&gt;Next are two examples using 3 units in the RNN layer, so that now the sequences are represented in ${\mathbb R}^3$. They both use data generated from a 5-state HMM, but with different state configurations:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;
becbcbyooooeceyyjyywccbbbnenennneybooyyywbooobnbyobooobeewyoooywyjwjjywoeeooooooooonbbybbeennnbbccbwyybooennoooooooooooooooooooooooyjwjbbbbbeoobbybeeebyybenegjywyywygenyyybbceennennnnnwjyybbboooecccbb
&lt;/tt&gt;&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex3_3_5_2_3D_bystate.png&quot; alt=&quot;&quot; /&gt;
    
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex3_3_5_2_families.png&quot; alt=&quot;&quot; /&gt;
    
  
  
&lt;/figure&gt;

&lt;p&gt;Here, we see that ‘confusion’ between HMM states is well represented in the RNN. The least confused HMM state is state 1, which is uniquely distinguished by outputting only ‘o’, and is coloured blue in the 3-dimensional plot. The most confused state is 4 (red). But remember that the RNN is not trying to distinguish the HMM states — it knows nothing about them — it is simply representing the observed structure of the output sequence. Of course, this structure reflects both the hidden states and their output distributions.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;
kqxvklljguguuuybywfrywrmkvbygglyuuguggggubjljwrwpulljuulrwwrrwugugguuummlppkzlrguupkpkpbytyupkxubuugtbyywhwuvxwrpvupppvybwlwrdpubgyuyxuuuugupxpuxuprwzfbjujljullugukwdzwrtbbbbtbybbbbtuugwfwffuguugbujbb
&lt;/tt&gt;&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex4_3_5_1_3D_bystate.png&quot; alt=&quot;&quot; /&gt;
    
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex4_3_5_1_families.png&quot; alt=&quot;&quot; /&gt;
    
  
  
&lt;/figure&gt;

&lt;p&gt;This last example has the most confusion among HMM states, and that is reflected in the 3-dimensional plot. Nevertheless, if we do a dimensional reduction to the plane using $t$-SNE for this example, we see that the separation is in fact still pretty good:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-11-01/ex4_3_5_1_tsne_bystate.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;How well do these RNN models answer problem 1 above? How well do they predict sequence outputs? Let’s focus on the last example.&lt;/p&gt;

&lt;p&gt;First note that the way we &lt;strong&gt;don’t&lt;/strong&gt; want to assess the predictive power of the model is to measure symbol error rate. That way, when we are dealing with inherently high entropy distributions, madness lies. In other words, in our situation each character is generated from a distribution which may be close to uniform, with multiple characters equally likely. Guessing the right one correctly may not tell us very much about the model — what we really need to know is that the model is giving us the right probability distribution.&lt;/p&gt;

&lt;p&gt;Since we have a God’s eye view of the data (it is generated from a Markov chain that we specified!) we know exactly what the right probability distribution is. That is, we know the (HMM) state transition matrix $P$ and the emission matrix $F$ (whose rows are the character distributions conditional on the HMM state). As we observe a sequence generated by the HMM, we can compute the ‘alpha’ vector as we go (which gives the joint probability of a given HMM state with the characters observed so far). Multiplying the alpha (row) vector on the right by the product $PF$ gives the distribution of the next character conditional on the sequence so far observed — exactly what we want.&lt;/p&gt;

&lt;p&gt;The RNN knows nothing about the HMM states, but is also outputting, as a softmax, its estimate of the same conditional distribution at each time step. The correct measure of its performance, therefore, as is how close this softmax is to the HMM-derived distribution.&lt;/p&gt;

&lt;p&gt;Here’s the comparison for the last example above. On the left I’ve plotted, for all possible next characters at all time steps in a test sequence a few thousand long, the predicted RNN (softmax) probability against the actual (HMM) probability. The histogram on the right shows the (mean per time-step) absolute value of the difference:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-11-01/ex4_3_5_1_prob_comparison.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;What we see is that the median discrepancy is around 1%. Not bad! Just for good measure, here’s a slightly hard example, with 12 well-mixed HMM states and using a 16-unit RNN (so the coloured plot is a $t$-SNE reduction from ${\mathbb R}^{16}$):&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;
cjydlrmppyygmmmyvprcchcmchauncanpngehshssllshhdolliaicaobdnnnabylroloddpgrbgprpyrpysyxzffefvzvzzewhsabgppyggehlpppyyalolnlddlvebnpbanbnfzebxvvfvbanbnnnoldlnnpnpabwbooupgppubvvfzvvviiaihhspphovvvveshlh
&lt;/tt&gt;&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex5_16_12_2_bystate.png&quot; alt=&quot;&quot; /&gt;
    
  
    
      &lt;img src=&quot;/assets/img/2019-11-01/ex5_16_12_1_families.png&quot; alt=&quot;&quot; /&gt;
    
  
  
&lt;/figure&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-11-01/ex5_16_12_2_prob_comparison.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We get a comparable performance, with median discrepancy around 1%.&lt;/p&gt;

&lt;p&gt;We can (and should!) also ask how these predicted probabilities (by HMM or by RNN) compare with what we actually observe. (This is after all a test we can always apply, not just for toy Markov-generated data.)&lt;/p&gt;

&lt;p&gt;In the plot below I’ve divided the interval $[0,1]$ into 20 bins. For each bin I’ve counted the proportion of character predictions with (HMM or RNN) probability in this range that actually happen. In other words, we’d like to see, if the model is well calibrated, that around 20% of prediction at 0.2 actually happen, and so on. I’ve plotted these proportions (blue for HMM, red for RNN), and the closer the plot to the diagonal, the better calibrated the model:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-11-01/ex5_16_12_1_calibration_no_dropout.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(Incidentally, being just above the diagonal is actually correct, because the observed proportion has been plotted against the bottom of the corresponding bin range.)&lt;/p&gt;

&lt;p&gt;This shows where the RNN performance is weaker. But up to this point I haven’t said anything about the details of training the RNN. In particular, it’s common to use dropout to prevent overfitting, and the calibration plot above was actually the result of training the RNN without any dropout. Since this calibration is a good measure of the RNN performance, we should compare with an RNN trained with dropout (20% on the dense connections between layers):&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-11-01/ex5_16_12_2_calibration_dropout_0pt2.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;So now the RNN is pretty well spot on.&lt;/p&gt;

&lt;p&gt;What’s needed now is a mathematical analysis to explain the observations above, to tell us how they will scale, how they will extend to high-order dependencies, and how the RNN performance will depend on architecture and training parameters.&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="data_science" /><summary type="html">Modern language models derive their power from big data and big compute – but also, ultimately, from the Unreasonable Effectiveness of Recurrent Neural Networks described by Andrej Karpathy (and many others) a decade or so ago. This post is more low-brow than Karpathy’s — I wanted to explore a little bit how RNNs perform on some carefully controlled toy data: specifically on sequences generated from Markov chains.</summary></entry><entry><title type="html">Mean-squared error versus cross-entropy</title><link href="http://localhost:4000/data_science/crossent/" rel="alternate" type="text/html" title="Mean-squared error versus cross-entropy" /><published>2019-10-01T00:00:00+00:00</published><updated>2019-10-01T00:00:00+00:00</updated><id>http://localhost:4000/data_science/crossent</id><content type="html" xml:base="http://localhost:4000/data_science/crossent/">&lt;p&gt;&lt;a href=&quot;https://billox.shinyapps.io/crossent/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/assets/img/2019-10-01/crossent.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This post is about multinomial probability distributions $p = (p_1, \ldots ,p_N)$ where $\sum_i p_i = 1$.&lt;/p&gt;

&lt;p&gt;We often need to compare two such distributions using a ‘distance’ function $d(p,q)$. For example (and the motivation for this blog post was a recent conversation about this case), the error that gets back-propagated when we train a neural network classifier: $p$ is the one-hot vector specifying the class of a training example, and $q$ is the corresponding soft-max output of the network.&lt;/p&gt;

&lt;p&gt;For this, one might use the good old Euclidean metric
\[
d_{\rm Eucl}(p,q) = \left( \sum_i | p_i - q_i |^2 \right)^\frac{1}{2}.
\]&lt;/p&gt;

&lt;p&gt;Alternatively, one can use cross-entropy (not a metric, but motivated from information theory)
\[
d_{\rm CE}(p,q) = \sum_i - p_i \log q_i .
\]&lt;/p&gt;

&lt;p&gt;Discussing these with a colleague recently, I claimed that the Euclidean error is ‘wrong’ because it’s translation-invariant, and this is not appropriate in the space of multinomial distributions, which is a simplex (and therefore bounded). But I thought afterwards it would be a good idea to make a visualisation to demonstrate this, and to understand just how the two metrics compare.&lt;/p&gt;

&lt;p&gt;For $N=3$ this is easy to do because the simplex is a triangle. In the interactive demo (linked to the image above),
select a distribution $p$ by clicking somewhere in the left triangle: the middle and right plots will then show a heat map of ‘distance’ to that point in cross-entropy and in the Euclidean metric respectively.&lt;/p&gt;

&lt;p&gt;What we see is that the Euclidean metric does not know about the geometry of the space of distributions. Cross-entropy, on the other hand, grows to infinity as the second argument $q$ goes to the boundary. So, for example, if $p=(0.9,0.05,0.05)$ and $q=(1,0,0)$, then $p$ and $q$ are very close in the Euclidean metric, but infinitely far apart in cross-entropy.&lt;/p&gt;

&lt;p&gt;(But note that this is not the case if $p$ and $q$ are reversed!! Cross-entropy is not symmetric, and is even linear in the first argument. You can see the effect of swapping direction of the distance measure using the menu below the middle plot.)&lt;/p&gt;

&lt;p&gt;On the other hand, we do see that for distributions that stay away from the boundary — that is, for those with higher entropy — mean squared error is a surprisingly good approximation to cross-entropy, at least for $N=3$.&lt;/p&gt;

&lt;p&gt;Of course, when we’re training neural networks, it’s precisely the low entropy, boundary distributions that matter. It’s the vertex points (one-hot distributions) that we use for training — in the vicinity of which cross-entropy is highly sensitive, while the Euclidean metric is blunt and unobservant.&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="data_science" /><summary type="html"></summary></entry><entry><title type="html">A word2vec map of Wikipedia words</title><link href="http://localhost:4000/data_science/wikiwords/" rel="alternate" type="text/html" title="A word2vec map of Wikipedia words" /><published>2019-09-01T00:00:00+00:00</published><updated>2019-09-01T00:00:00+00:00</updated><id>http://localhost:4000/data_science/wikiwords</id><content type="html" xml:base="http://localhost:4000/data_science/wikiwords/">&lt;p&gt;&lt;a href=&quot;https://billox.shinyapps.io/wikiwords/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/assets/img/2019-09-01/wikimap.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is an update of a little exercise, a few years ago, in making my own &lt;em&gt;word2vec&lt;/em&gt; map of Wikipedia words.&lt;/p&gt;

&lt;p&gt;At that time, I started with 72,000 Wikipedia words and their &lt;em&gt;word2vec&lt;/em&gt; representations in 200-dimensional space. In order to visualise that cloud of points, I made a graph by first binning the point cloud (using the word frequency) and then clustering within each bin. An individual cluster became a graph vertex, and two clusters with a word in common defined an edge.&lt;/p&gt;

&lt;p&gt;Visualisation of the point cloud then used standard force-directed graph layouts to plot the graph, followed by drilling down into vertex subsets to explore the word sets.&lt;/p&gt;

&lt;p&gt;This post shows an alternative method for the graph layout: since the vertices correspond to clusters of points in a vector space, we can represent them by the centroids of those clusters, and then map them into the plane by using a $t$-SNE projection of the centroids.&lt;/p&gt;

&lt;p&gt;The plot above links to an interactive demo that shows the result. I’ve suppressed drawing of the edges because they are often long-range and less helpful to the eye. (Though they are meaningful: they represent the many words which inhabit multiple semantic communities.) This view is better at revealing natural large-scale clusters in the data — which is what we really want. For example, along the bottom edge of the plot there are four main clusters, which are roughly foodie, zoological, biochemical and medical. (And I wiled away a train journey annotating the plot above with the different semantic regions!)&lt;/p&gt;

&lt;p&gt;The colour denotes word frequency: white for common words to red for rare words.&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="data_science" /><summary type="html"></summary></entry><entry><title type="html">A demo of the $t$-test</title><link href="http://localhost:4000/data_science/demo-of-the-ttest/" rel="alternate" type="text/html" title="A demo of the $t$-test" /><published>2019-08-01T00:00:00+00:00</published><updated>2019-08-01T00:00:00+00:00</updated><id>http://localhost:4000/data_science/demo-of-the-ttest</id><content type="html" xml:base="http://localhost:4000/data_science/demo-of-the-ttest/">&lt;p&gt;&lt;a href=&quot;https://billox.shinyapps.io/ttest/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/assets/img/2019-08-01/ttest.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This one was for my daughter, who at the time was learning about the $t$-test.&lt;/p&gt;

&lt;p&gt;The demo linked above shows how the two-sample (Welch) $t$-test works. The idea is that there are two random variables (or populations) $A$ and $B$, each normally distributed, and we want to test whether these populations have the same mean. So we take a sample from each population. (Shown in the yellow boxplots. The red bar in the boxplot is the sample mean.)&lt;/p&gt;

&lt;p&gt;Suppose these samples have size $n_A$, $n_B$ respectively, and that they have means $\overline{X}_A$, $\overline{X}_B$, and sample variances $s_A^2$, $s_B^2$. Let
\[
t = \frac{\overline{X}_A - \overline{X}_B}{s}
\]
where
\[
s = \sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}.
\]&lt;/p&gt;

&lt;p&gt;Then one can show that &lt;strong&gt;if&lt;/strong&gt; the (unknown) population means are the same (that’s our null hypothesis) then the variable $t$, as we repeatedly draw samples, is distributed with a $t$-distribution.&lt;/p&gt;

&lt;p&gt;This is what the main plot shows. For each experiment, a sample is drawn from each of $A$ and $B$, and the value of $t$ is marked as a blue cross on the horizontal axis. For comparison, the grey histogram shows how the blue cross would be distributed over 1000 such experiments. So you can see how typical the value of $t$ is for the given population parameters. The red curve is the $t$-distribution under our null hypothesis. So you can also see how typical the blue cross is with respect to this hypothesis. Note that, with the starting values of the population parameters, the means are equal, so the null hypothesis is met and the red curve matches well to the grey histogram. But explore what happens as the means are moved apart.&lt;/p&gt;

&lt;p&gt;The red region in the tail of the $t$-distribution from our blue cross has area which equals the probability, under the null hypothesis, of finding this or a more extreme result. This is the $p$-value of the test.&lt;/p&gt;

&lt;p&gt;To be precise, the red curve plots the $t$-density
\[
f(x) = 
\frac{\Gamma {d+1 \choose 2}}{\sqrt{d \pi} \Gamma {d \choose 2}}\left( 1 + \frac{x^2}{d}\right)^{-\frac{d+1}{2}}
\]
where $d$ is a parameter called the number of degrees of freedom. If one does the maths to derive the distribution of the $t$-statistic under our null hypothesis, one finds a messy formula for $d$:
\[
d = \frac{s^4}{\frac{(s_A^2 / n_A)^2}{n_A - 1} + \frac{(s_B^2 / n_B)^2}{n_B - 1}}
\]
So as the sample sizes increase, so does $d$, and the red curve $f(x)$ becomes more concentrated at 0.&lt;/p&gt;

&lt;p&gt;At the bottom of the plot is the output of the R function &lt;em&gt;t.test()&lt;/em&gt; for the given samples. It shows, in particular, the value of $t$ and the $p$-value. The demo tries to make this output intuitive — how does the $p$-value respond as we vary the population mean and variance, or the sample sizes?&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="data_science" /><summary type="html"></summary></entry><entry><title type="html">A topological trick for data visualisation</title><link href="http://localhost:4000/data_science/topological-trick-for-data-visualisation/" rel="alternate" type="text/html" title="A topological trick for data visualisation" /><published>2019-07-01T00:00:00+00:00</published><updated>2019-07-01T00:00:00+00:00</updated><id>http://localhost:4000/data_science/topological-trick-for-data-visualisation</id><content type="html" xml:base="http://localhost:4000/data_science/topological-trick-for-data-visualisation/">&lt;p&gt;&lt;a href=&quot;https://billox.shinyapps.io/zip-topological-viz/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/assets/img/2019-07-01/zip2.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How do hand-written digits arrange themselves in pixel space?&lt;/p&gt;

&lt;p&gt;The plot above (and the interactive version linked to it) illustrates the ‘mapper’ construction described in &lt;a href=&quot;https://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf&quot;&gt;Gunnar Carlsson, Topology and Data, Bull Amer Math Soc 46 (2009)&lt;/a&gt; — here applied to the well known ‘Zip’ data set consisting of 7,291 hand-written digits.&lt;/p&gt;

&lt;p&gt;The original data lives in 256-dimensional (8 x 8) pixel space. The idea of the construction is to partition the data points into overlapping bins — thought of as an ‘open cover’ — and to perform heirarchical clustering within each bin. A graph is then constructed whose vertices are the resulting clusters across all bins, and whose edges say that a pair of bins has one or more points in common. In the interactive plot you can control the resolution via two variables: the number of bins and the number of clusters extracted within each bin.&lt;/p&gt;

&lt;p&gt;How should you choose these tuning parameters? There is a trade-off between number of clusters (i.e. the number of graph vertices) and the amount of variation of digits within each cluster. You want both of these to be low.&lt;/p&gt;

&lt;p&gt;You can test this by choosing a random vertex – the left-panel then shows a sample of images from the cluster of data points corresponding to this vertex.&lt;/p&gt;

&lt;p&gt;So where do the bins come from? Following the topological analogy, we pull back open intervals from the real line under a suitable ‘continuous’ function. For this function, I’ve taken local density of the point cloud — measured by counting neighbouring points within a fixed radius.&lt;/p&gt;

&lt;p&gt;There is, however, a caveat … The curse of dimension means that local density is not very sensitive in 256 dimensions. So I’ve actually performed a dimensional reduction first (down to 6 dimensions, in fact). There are various ways one can do this — I’ve used a Fiedler embedding from the nearest-neighbour graph in pixel space (but I’m brushing that under the carpet for now).&lt;/p&gt;</content><author><name>Bill Oxbury</name></author><category term="data_science" /><summary type="html"></summary></entry></feed>